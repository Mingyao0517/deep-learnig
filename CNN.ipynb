{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ab2292",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment_1/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b20982",
   "metadata": {},
   "source": [
    "\n",
    "# Student : Mingyao Zhao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as plt\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfb295",
   "metadata": {},
   "source": [
    "### Training data set\n",
    "\n",
    "\n",
    "For Assignment 1 you need to use a specific data set prepared using images from the [Omniglot dataset](https://github.com/brendenlake/omniglot). The provided training data set contains images of handwritten characters of size (28,28). \n",
    "\n",
    "\n",
    "\n",
    "For training data, the dataset contains 10000 sets of 6 images each. Each set consists of 5 support images and 1 query image. In each set, the first five columns are support images and the last one is a query image.\n",
    "\n",
    "For training labels, the dataset contains 10000 sets of 5 binary flags for support images. 1 indicates the same character is given in the query image and 0 means not. For example, a label [1,0,0,1,1] means the support images with index 0,3,4 are the same character of query image.\n",
    "\n",
    " \n",
    " \n",
    "The following cell provides code that loads the data from hardcoded URLs.You can use the code in this cell to load the dataset or download the data set from the given URLs to your local drive (or your Google drive) and modify the code to load the data from another location. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_arr_from_url(url):\n",
    "    \"\"\"\n",
    "    Loads a numpy array from surfdrive. \n",
    "    \n",
    "    Input:\n",
    "    url: Download link of dataset \n",
    "\n",
    "    Outputs:\n",
    "    dataset: numpy array with input features or labels\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return np.load(io.BytesIO(response.content)) \n",
    "    \n",
    "    \n",
    "    \n",
    "#Downloading may take a while..\n",
    "train_data = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/4OXkVie05NPjRKK/download')\n",
    "train_label = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/oMLFw60zpFX82ua/download')\n",
    "\n",
    "print(f\"train_data shape: {train_data.shape}\")\n",
    "print(f\"train_label shape: {train_label.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb937392",
   "metadata": {},
   "source": [
    "Now, we plot the first 5 cases in the training dataset. The last column corresponds with the query images of each task. All other images are support images. The image enclosed in a red box denotes the target image that your model should be able to recognize as the same class as the query image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_case(caseID,train_data,labels):\n",
    "    \"\"\"\n",
    "    Plots a single sample of the query dataset\n",
    "    \n",
    "    Inputs\n",
    "    caseID: Integer between 0 and 99, each corresponding to a single sample in the query dataset \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    support_set,queries = np.split(train_data, [5], axis=1)\n",
    "    \n",
    "    f, axes = plt.subplots(1, 6, figsize=(20,5))\n",
    "\n",
    "    # plot anchor image\n",
    "    axes[5].imshow(queries[caseID, 0])\n",
    "    axes[5].set_title(f\"Query image case {caseID}\", fontsize=15)\n",
    "\n",
    "    # show all test images images \n",
    "    [ax.imshow(support_set[caseID, i]) for i, ax in enumerate(axes[0:-1])]\n",
    "\n",
    "\n",
    "    # Add the patch to the Axes\n",
    "    for ind in np.where(labels[caseID]==True)[0]:\n",
    "        axes[ind].add_patch(Rectangle((0,0),27,27,linewidth=2, edgecolor='r',facecolor='none'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03538ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_case(caseID,train_data,train_label) for caseID in range(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f2383",
   "metadata": {},
   "source": [
    "### Query data set\n",
    "\n",
    "For this task you need to use the following query data set. The dataset contains 1000 sets of 6 images each. The images are also of hand written characters, however these characters are not present in the training data set. The characters in the query data set all come from the Greek alphabet that is not part of the set of alphabets in the training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#Downloading may take a while..\n",
    "test_data = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/06c34QVUr69CxWY/download')\n",
    "test_label = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/LQIH1CW7lfDXevk/download')\n",
    "\n",
    "print(f\"test_data shape: {test_data.shape}\")\n",
    "print(f\"test_label shape: {test_label.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb3580",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[plot_case(caseID,test_data,test_label) for caseID in range(5)] ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc32db2",
   "metadata": {},
   "source": [
    "### Build pytorch dataset and dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf31f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):  # get the label and data by index\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y\n",
    "    \n",
    "train_dataset=MyDataset(train_data,train_label)\n",
    "test_dataset=MyDataset(test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6fdfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Model Definition ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "class Mynetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super(Mynetwork, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            # Lambda(preprocess),\n",
    "            nn.Conv2d(1, 32, 3, padding=1),  # 32@28*28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),    # 64@28*28\n",
    "            nn.MaxPool2d(2),   # 64@14*14\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), # 128@14*14\n",
    "            # nn.MaxPool2d(2), \n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),   # 128@14*14\n",
    "            nn.MaxPool2d(2), # 128@7*7\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(), # 256@7*7\n",
    "            # nn.MaxPool2d(2), \n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),   # 256@7*\n",
    "            nn.MaxPool2d(2), # 256@3*3\n",
    "\n",
    "            #flatten the tensor\n",
    "            Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "            nn.Linear(2304, 1024),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(1024, 30),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        # conv layers\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss definition(using the triplet loss for training dataset)\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplets loss\n",
    "    Takes a batch of embeddings and corresponding labels.\n",
    "    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n",
    "    triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        ap_distances = (anchor - positive).pow(2).sum(1)  \n",
    "        an_distances = (anchor - negative).pow(2).sum(1) \n",
    "        losses = F.relu(ap_distances - an_distances + self.margin)  # The gap is greater than the margin\n",
    "        return losses.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a9501",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build Sample Selector ##\n",
    "class RandomTripletSelector():\n",
    "    \"\"\"\n",
    "    Select random negative example for each positive pair to create triplets\n",
    "    \"\"\"\n",
    "    def __init__(self,input):\n",
    "        super(RandomTripletSelector, self).__init__()\n",
    "        self.input=input\n",
    "        self.data=self.input[:][0]  \n",
    "        self.target=self.input[:][1]  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        anchor = torch.unsqueeze(self.input[item][0][-1], 0)\n",
    "        positive_index=[]\n",
    "        negative_index=[]\n",
    "        for i ,label_pos in enumerate(self.input[item][1]):\n",
    "            if label_pos==1:\n",
    "                positive_index.append(i)\n",
    "            elif label_pos==0:\n",
    "                negative_index.append(i)\n",
    "        positive_indices=np.random.choice(positive_index)\n",
    "        negative_indices=np.random.choice(negative_index)\n",
    "        positive=torch.unsqueeze(self.input[item][0][positive_indices],0)\n",
    "        negative=torch.unsqueeze(self.input[item][0][negative_indices],0)\n",
    "        return anchor,positive,negative\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_Dou(model, train_dataset, test_dataset, num_epochs, loss_fuc):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        tra, val = train_test_split(train_dataset, test_size=0.3, shuffle=True)\n",
    "        train_ds = RandomTripletSelector(tra)\n",
    "        val_ds = RandomTripletSelector(val)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_ds ,batch_size=10)\n",
    "        val_loader = torch.utils.data.DataLoader(val_ds ,batch_size=10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        model.train()  \n",
    "        print(\"Starting epoch \" + str(epoch+1))\n",
    "        for (anchor, positive, negative) in train_loader: \n",
    "            # Forward\n",
    "            # print(anchor.shape)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "            anchor = anchor.to(device)\n",
    "            pred_anchor = model(anchor)\n",
    "            pred_positive = model(positive)\n",
    "            pred_negative = model(negative)\n",
    "            loss = loss_fuc(pred_anchor, pred_positive, pred_negative)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()  \n",
    "            loss.backward()  \n",
    "            optimizer.step() \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        val_running_loss = 0.0\n",
    "        \n",
    "        #check validation loss after every epoch\n",
    "        with torch.no_grad():  \n",
    "            model.eval()  \n",
    "            for (anchor, positive, negative) in val_loader:\n",
    "                positive = positive.to(device)\n",
    "                negative = negative.to(device)\n",
    "                anchor = anchor.to(device)\n",
    "                pred_anchor = model(anchor)\n",
    "                pred_positive = model(positive)\n",
    "                pred_negative = model(negative)\n",
    "\n",
    "                loss = loss_fuc(pred_anchor,pred_positive,pred_negative)\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_running_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.8f}'\n",
    "            .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
    "    print(\"Finished Training\")  \n",
    "    return train_losses, val_losses \n",
    "\n",
    "def plot_loss(train_loss, val_loss,epoche):\n",
    "    \"\"\"\n",
    "    plot the loss change during the training precedure\n",
    "    \"\"\"\n",
    "    plt.title(\"Train and validation loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    epoche_range=[i+1 for i in range(epoche)]\n",
    "    plt.plot(epoche_range, train_loss)\n",
    "    plt.plot(epoche_range, val_loss)\n",
    "    plt.legend(['train loss', 'validation loss'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model and plot loss\n",
    "epoches = 30\n",
    "model = Mynetwork()\n",
    "\n",
    "Loss_func=TripletLoss(margin = 0.5)\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "train_losses,val_losses=train_Dou(model, train_dataset, test_dataset, epoches, Loss_func)\n",
    "plot_loss(train_loss=train_losses,val_loss=val_losses,epoche=epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64252d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation ##\n",
    "def similarity_eculide(x,y):\n",
    "    \"\"\"\n",
    "    calculate the distance between x and y\n",
    "    \"\"\"\n",
    "    similarity = torch.cdist(x, y, p=2.0)\n",
    "    return similarity\n",
    "   \n",
    "\n",
    "def confusion(l1,l2):  # l2: predicted, l1: ground truth\n",
    "    \"\"\"\n",
    "    get the number in confusion matrix\n",
    "    \"\"\"\n",
    "    l1 = torch.Tensor(l1)\n",
    "    l2 = torch.Tensor(l2)\n",
    "    TP = torch.sum(((l2 == 1) & (l1 == 1)) == 1).item()\n",
    "    FN = torch.sum(((l2 == 0) & (l1 == 1)) == 1).item()\n",
    "    FP = torch.sum(((l2 == 1) & (l1 == 0)) == 1).item()\n",
    "    TN = torch.sum(((l2 == 0) & (l1 == 0)) == 1).item()\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def compareList(l1,l2):\n",
    "    if(l1 == l2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def confusion_cal(threshold, dataset):\n",
    "    \"\"\"\n",
    "    cal the TPR, FPR, Accuracy and Complete_Accuracy\n",
    "    \"\"\"\n",
    "    result=[]\n",
    "    for i in range(len(dataset[:][0])):\n",
    "        sub_result=[]\n",
    "        for j in range(5):\n",
    "            dis = similarity_eculide(model.forward(dataset[i][0][-1].view(-1, 1, 28, 28).to(device)), model.forward(dataset[i][0][j].view(-1, 1, 28, 28).to(device)))\n",
    "            if dis >= threshold:\n",
    "                value = 0\n",
    "            else:\n",
    "                value = 1\n",
    "            sub_result.append(value)\n",
    "        result.append(sub_result)\n",
    "\n",
    "    target=dataset.targets.tolist()\n",
    "    value1=np.array([confusion(target[i],result[i]) for i in range(len(dataset))])\n",
    "\n",
    "    TPR = sum(value1[:,0])/(sum(value1[:,0]) + sum(value1[:,3]) + 1e-5)\n",
    "    FPR = sum(value1[:,2])/(sum(value1[:,1]) + sum(value1[:,2]) + 1e-5)\n",
    "    ACC = sum(value1[:,0] + value1[:,1])/(sum(value1[:,0]) + sum(value1[:,1]) + sum(value1[:,2]) + sum(value1[:,3]) + 1e-5)\n",
    "\n",
    "    value2=[compareList(target[i],result[i]) for i in range(len(dataset))]\n",
    "    complete_acc=sum(value2)/len(value2)\n",
    "\n",
    "    return TPR, FPR, ACC, complete_acc,target,result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348bbc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the best threshold\n",
    "def range_with_floats(start, stop, step):\n",
    "    while stop > start:\n",
    "        yield start\n",
    "        start += step\n",
    "\n",
    "acc_dou=[confusion_cal(threshold=i,dataset=train_dataset) for i in range_with_floats(0,3.0,0.1)]\n",
    "threhlod=[i for i in range_with_floats(0,3.0,0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best threshold based on ROC\n",
    "def plot_roc(TPR,FPR):\n",
    "    \"\"\"\n",
    "    plot the roc curve\n",
    "    \"\"\"\n",
    "    plt.title(\"ROC curve\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.plot(FPR, TPR)\n",
    "    plt.show()\n",
    "\n",
    "plot_roc([x[0] for x in acc_dou], [x[1] for x in acc_dou])\n",
    "\n",
    "a1a1 = np.array([x[0] for x in acc_dou])\n",
    "b1b1 = np.array([x[1] for x in acc_dou])\n",
    "\n",
    "best_threshold=np.argmax(a1a1 - b1b1)\n",
    "print('The accuarcy with different threshold')\n",
    "print('The threhlod with the best ROC is :', threhlod[best_threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f84954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best threshold based on ACC\n",
    "def plot_acc(ACC,threhlod):\n",
    "    plt.title(\"ACC curve\")\n",
    "    plt.xlabel(\"Threhlod\")\n",
    "    plt.ylabel(\"ACC\")\n",
    "    plt.plot(threhlod, ACC)\n",
    "    plt.show()\n",
    "\n",
    "plot_acc([x[2] for x in acc_dou], threhlod)\n",
    "\n",
    "a1a1 = np.array([x[2] for x in acc_dou])\n",
    "\n",
    "best_threshold=np.argmax(a1a1)\n",
    "print('The accuarcy with different threshold')\n",
    "print('The threhlod with the best accuarcy is :', threhlod[best_threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best threshold based on complete ACC\n",
    "def plot_cacc(Complete_ACC,threhlod):\n",
    "    plt.title(\"Complete ACC curve\")\n",
    "    plt.xlabel(\"Threhlod\")\n",
    "    plt.ylabel(\"Complete_ACC\")\n",
    "    plt.plot(threhlod, Complete_ACC)\n",
    "    plt.show()\n",
    "\n",
    "plot_cacc([x[3] for x in acc_dou], threhlod)\n",
    "\n",
    "a1a1 = np.array([x[3] for x in acc_dou])\n",
    "\n",
    "best_threshold=np.argmax(a1a1)\n",
    "print('The accuarcy with different threshold')\n",
    "print('The threhlod with the best accuarcy is :', threhlod[best_threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ef99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# therefore, we get the best threshold\n",
    "best_threshold = 1.5\n",
    "#evulation on train set\n",
    "acc_final = confusion_cal(threshold=best_threshold,dataset=train_dataset)\n",
    "print('The accuarcy on train set is :', acc_final[2])\n",
    "print('The complete accuarcy on train set is :', acc_final[3])\n",
    "print('TPR on train set is:', acc_final[0])\n",
    "print('FPR on train set is:', acc_final[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3275fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evulation on test set\n",
    "acc_final = confusion_cal(threshold=best_threshold,dataset=test_dataset)\n",
    "print('The accuarcy on test set is :', acc_final[2])\n",
    "print('The complete accuarcy on test set is :', acc_final[3])\n",
    "print('TPR on test set is:', acc_final[0])\n",
    "print('FPR on test set is:', acc_final[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0555de903002cdf4a46050f793c94d982502fe7f47d0d08f577f35cd716562b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
